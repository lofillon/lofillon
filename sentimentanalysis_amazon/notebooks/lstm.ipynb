{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import os\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "color = sns.color_palette()\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset\n",
        "DATA_PATH = Path(\"../data/raw/french_to_english_product.csv\")\n",
        "if not DATA_PATH.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Missing dataset: {DATA_PATH}. Run `notebooks/knn_naive_bayes.ipynb` first (it downloads and saves the CSV), or place the file there manually.\"\n",
        "    )\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "\n",
        "# Split the dataset\n",
        "whole_df_fr_review = df[['review', 'rating']]\n",
        "whole_df_en_translation = df[['translation', 'rating']]\n",
        "\n",
        "# Split the dataset into 20% train and 80% test\n",
        "train_df_en_translation, test_df_en_translation = train_test_split(whole_df_en_translation, test_size=0.20, random_state=42)\n",
        "\n",
        "train_df_fr_review, test_df_fr_review = train_test_split(whole_df_fr_review, test_size=0.20, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation, Flatten\n",
        "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, SpatialDropout1D\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### French LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Words cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fr_data_cleaning(raw_data):\n",
        "    raw_data = raw_data.translate(str.maketrans('', '', string.punctuation + string.digits))\n",
        "    words = raw_data.lower().split()\n",
        "    stops = set(stopwords.words(\"french\"))\n",
        "    useful_words = [w for w in words if w not in stops]\n",
        "    return \" \".join(useful_words)\n",
        "train_df_fr_review['review']=train_df_fr_review['review'].apply(fr_data_cleaning)\n",
        "test_df_fr_review['review']=test_df_fr_review['review'].apply(fr_data_cleaning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "y = train_df_fr_review[\"rating\"].values\n",
        "y_test = test_df_fr_review[\"rating\"].values\n",
        "fr_train = train_df_fr_review[\"review\"]\n",
        "fr_test = test_df_fr_review[\"review\"]\n",
        "\n",
        "y = np.where(y <= 2, 0, np.where(y == 3, 1, 2))\n",
        "\n",
        "y_test = np.where(y_test <= 2, 0, np.where(y_test == 3, 1, 2))\n",
        "\n",
        "y = np.array(y)\n",
        "\n",
        "max_features = 6000\n",
        "tokenizer = Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(list(fr_train))\n",
        "list_fr_tokenized_train = tokenizer.texts_to_sequences(fr_train)\n",
        "list_fr_tokenized_test = tokenizer.texts_to_sequences(fr_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Define the maximum sequence length (adjust based on your dataset)\n",
        "max_sequence_length = 300\n",
        "\n",
        "# Pad the sequences\n",
        "X_fr_train = pad_sequences(list_fr_tokenized_train, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "X_fr_test = pad_sequences(list_fr_tokenized_test, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "\n",
        "print(\"Shape of X_train:\", X_fr_train.shape)\n",
        "print(\"Shape of X_test:\", X_fr_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epochs, logs={}):\n",
        "        if logs.get('accuracy') > 0.95:\n",
        "            print('\\n Stopped Training!\\n')\n",
        "            self.model.stop_training = True\n",
        "\n",
        "def train_model(model, model_name, n_epochs, batch_size, X_data, \n",
        "                y_data, validation_split):    \n",
        "    checkpoint_path = model_name+\"_cp-{epoch:04d}.keras\"\n",
        "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                    verbose=1)\n",
        "    callbacks = myCallback()\n",
        "    history = model.fit(\n",
        "        X_data,\n",
        "        y_data,\n",
        "        steps_per_epoch=batch_size,\n",
        "        epochs=n_epochs,\n",
        "        validation_split=validation_split,\n",
        "        verbose=1,\n",
        "        callbacks=[cp_callback]\n",
        "    )\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_graph(history):\n",
        "    plt.plot(history.history['accuracy'], 'b')\n",
        "    plt.plot(history.history['val_accuracy'], 'r')\n",
        "    plt.title('Model Accuracy'),\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a function to create the model with hyperparameters\n",
        "def Model_FR(embed_size, dense_units, dropout_rate, activation, optimizer):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(max_features, embed_size))\n",
        "    model.add(LSTM(50, return_sequences=True))\n",
        "    model.add(GlobalMaxPool1D())\n",
        "    model.add(Dense(dense_units, activation=activation))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(len(set(y)), activation=\"softmax\"))\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Manual hyperparameter optimization\n",
        "param_grid = {\n",
        "    'embed_size': [32, 64, 128],\n",
        "    'dense_units': [16, 32, 64],\n",
        "    'dropout_rate': [0.2, 0.5, 0.8],\n",
        "    'activation': ['relu', 'tanh'],\n",
        "    'optimizer': ['adam', 'sgd'],\n",
        "    'batch_size': [16, 32],\n",
        "    'epochs': [10, 20]\n",
        "}\n",
        "\n",
        "best_val_accuracy = 0\n",
        "best_params = {}\n",
        "\n",
        "for embed_size in param_grid['embed_size']:\n",
        "    for dense_units in param_grid['dense_units']:\n",
        "        for dropout_rate in param_grid['dropout_rate']:\n",
        "            for activation in param_grid['activation']:\n",
        "                for optimizer in param_grid['optimizer']:\n",
        "                    for batch_size in param_grid['batch_size']:\n",
        "                        for epochs in param_grid['epochs']:\n",
        "                            model = Model_FR(embed_size, dense_units, dropout_rate, activation, optimizer)\n",
        "                            history = model.fit(\n",
        "                                X_fr_train, y,\n",
        "                                batch_size=batch_size,\n",
        "                                epochs=epochs,\n",
        "                                validation_split=0.2,\n",
        "                                verbose=0\n",
        "                            )\n",
        "                            val_accuracy = max(history.history['val_accuracy'])\n",
        "                            if val_accuracy > best_val_accuracy:\n",
        "                                best_val_accuracy = val_accuracy\n",
        "                                best_params = {\n",
        "                                    'embed_size': embed_size,\n",
        "                                    'dense_units': dense_units,\n",
        "                                    'dropout_rate': dropout_rate,\n",
        "                                    'activation': activation,\n",
        "                                    'optimizer': optimizer,\n",
        "                                    'batch_size': batch_size,\n",
        "                                    'epochs': epochs\n",
        "                                }\n",
        "\n",
        "print(\"Best French Parameters:\", best_params)\n",
        "\n",
        "# Train the final model with the best parameters\n",
        "model_fr = Model_FR(\n",
        "    embed_size=best_params['embed_size'],\n",
        "    dense_units=best_params['dense_units'],\n",
        "    dropout_rate=best_params['dropout_rate'],\n",
        "    activation=best_params['activation'],\n",
        "    optimizer=best_params['optimizer']\n",
        ")\n",
        "history_fr = train_model(model_fr, \"model_fr\", best_params['epochs'], best_params['batch_size'], X_fr_train, y, 0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, confusion_matrix\n",
        "\n",
        "def evaluate_model_fr(model):\n",
        "  prediction = model.predict(X_fr_test)\n",
        "  y_pred = prediction.argmax(axis=1)  # Get the class with the highest probability\n",
        "  accuracy = accuracy_score(y_test, y_pred)\n",
        "  precision = precision_score(y_test, y_pred, average='macro')  # Use 'macro' for multi-class\n",
        "  recall = recall_score(y_test, y_pred, average='macro')  # Use 'macro' for multi-class\n",
        "  f1 = f1_score(y_test, y_pred, average='macro')  # Use 'macro' for multi-class\n",
        "  cf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "  print(f\"Accuracy: {accuracy:.3f}\")\n",
        "  print(f\"Precision: {precision:.3f}\")\n",
        "  print(f\"Recall: {recall:.3f}\")\n",
        "  print(f\"F1-score: {f1:.3f}\")\n",
        "  print(\"Confusion Matrix:\\n\", cf_matrix)\n",
        "\n",
        "  return accuracy, precision, recall, f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# After training your model_d, evaluate it:\n",
        "accuracy, precision, recall, f1 = evaluate_model_fr(model_fr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### English\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Words cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def en_data_cleaning(raw_data):\n",
        "    raw_data = raw_data.translate(str.maketrans('', '', string.punctuation + string.digits))\n",
        "    words = raw_data.lower().split()\n",
        "    stops = set(stopwords.words(\"english\"))\n",
        "    useful_words = [w for w in words if not w in stops]\n",
        "    return( \" \".join(useful_words))\n",
        "train_df_en_translation['translation']=train_df_en_translation['translation'].apply(en_data_cleaning)\n",
        "test_df_en_translation['translation']=test_df_en_translation['translation'].apply(en_data_cleaning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y = train_df_en_translation[\"rating\"].values\n",
        "y_test = test_df_en_translation[\"rating\"].values\n",
        "en_train = train_df_en_translation[\"translation\"]\n",
        "en_test = test_df_en_translation[\"translation\"]\n",
        "\n",
        "y = np.where(y <= 2, 0, np.where(y == 3, 1, 2))\n",
        "\n",
        "y_test = np.where(y_test <= 2, 0, np.where(y_test == 3, 1, 2))\n",
        "\n",
        "y = np.array(y)\n",
        "\n",
        "max_features = 6000\n",
        "tokenizer = Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(list(en_train))\n",
        "list_en_tokenized_train = tokenizer.texts_to_sequences(en_train)\n",
        "list_en_tokenized_test = tokenizer.texts_to_sequences(en_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Define the maximum sequence length (adjust based on your dataset)\n",
        "max_sequence_length = 300\n",
        "\n",
        "# Pad the sequences\n",
        "X_en_train = pad_sequences(list_en_tokenized_train, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "X_en_test = pad_sequences(list_en_tokenized_test, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "\n",
        "print(\"Shape of X_train:\", X_en_train.shape)\n",
        "print(\"Shape of X_test:\", X_en_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a function to create the model with hyperparameters\n",
        "def Model_EN(embed_size, dense_units, dropout_rate, activation, optimizer):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(max_features, embed_size))\n",
        "    model.add(LSTM(50, return_sequences=True))\n",
        "    model.add(GlobalMaxPool1D())\n",
        "    model.add(Dense(dense_units, activation=activation))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(len(set(y)), activation=\"softmax\"))\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Manual hyperparameter optimization\n",
        "param_grid_en = {\n",
        "    'embed_size': [32, 64, 128],\n",
        "    'dense_units': [16, 32, 64],\n",
        "    'dropout_rate': [0.2, 0.5, 0.8],\n",
        "    'activation': ['relu', 'tanh'],\n",
        "    'optimizer': ['adam', 'sgd'],\n",
        "    'batch_size': [16, 32],\n",
        "    'epochs': [10, 20]\n",
        "}\n",
        "\n",
        "best_val_accuracy_en = 0\n",
        "best_params_en = {}\n",
        "\n",
        "for embed_size in param_grid_en['embed_size']:\n",
        "    for dense_units in param_grid_en['dense_units']:\n",
        "        for dropout_rate in param_grid_en['dropout_rate']:\n",
        "            for activation in param_grid_en['activation']:\n",
        "                for optimizer in param_grid_en['optimizer']:\n",
        "                    for batch_size in param_grid_en['batch_size']:\n",
        "                        for epochs in param_grid_en['epochs']:\n",
        "                            model = Model_FR(embed_size, dense_units, dropout_rate, activation, optimizer)\n",
        "                            history = model.fit(\n",
        "                                X_en_train, y,\n",
        "                                batch_size=batch_size,\n",
        "                                epochs=epochs,\n",
        "                                validation_split=0.2,\n",
        "                                verbose=0\n",
        "                            )\n",
        "                            val_accuracy_en = max(history.history['val_accuracy'])\n",
        "                            if val_accuracy_en > best_val_accuracy_en:\n",
        "                                best_val_accuracy_en = val_accuracy_en\n",
        "                                best_params_en = {\n",
        "                                    'embed_size': embed_size,\n",
        "                                    'dense_units': dense_units,\n",
        "                                    'dropout_rate': dropout_rate,\n",
        "                                    'activation': activation,\n",
        "                                    'optimizer': optimizer,\n",
        "                                    'batch_size': batch_size,\n",
        "                                    'epochs': epochs\n",
        "                                }\n",
        "\n",
        "print(\"Best English Parameters:\", best_params_en)\n",
        "\n",
        "# Train the final model with the best parameters\n",
        "model_en = Model_EN(\n",
        "    embed_size=best_params_en['embed_size'],\n",
        "    dense_units=best_params_en['dense_units'],\n",
        "    dropout_rate=best_params_en['dropout_rate'],\n",
        "    activation=best_params_en['activation'],\n",
        "    optimizer=best_params_en['optimizer']\n",
        ")\n",
        "history_en = train_model(model_en, \"model_en\", best_params_en['epochs'], best_params_en['batch_size'], X_en_train, y, 0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, confusion_matrix\n",
        "\n",
        "def evaluate_model_en(model):\n",
        "  prediction = model.predict(X_en_test)\n",
        "  y_pred = prediction.argmax(axis=1)  # Get the class with the highest probability\n",
        "  accuracy = accuracy_score(y_test, y_pred)\n",
        "  precision = precision_score(y_test, y_pred, average='macro')  # Use 'macro' for multi-class\n",
        "  recall = recall_score(y_test, y_pred, average='macro')  # Use 'macro' for multi-class\n",
        "  f1 = f1_score(y_test, y_pred, average='macro')  # Use 'macro' for multi-class\n",
        "  cf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "  print(f\"Accuracy: {accuracy:.3f}\")\n",
        "  print(f\"Precision: {precision:.3f}\")\n",
        "  print(f\"Recall: {recall:.3f}\")\n",
        "  print(f\"F1-score: {f1:.3f}\")\n",
        "  print(\"Confusion Matrix:\\n\", cf_matrix)\n",
        "\n",
        "  return accuracy, precision, recall, f1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# After training your model_d, evaluate it:\n",
        "accuracy, precision, recall, f1 = evaluate_model_en(model_en)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "hEKKmzzBpVZ-"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
